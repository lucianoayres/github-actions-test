name: Install and Run Ollama

on:
    workflow_dispatch: # Allows running this workflow manually from the Actions tab

permissions:
    contents: read

jobs:
    run_ollama:
        runs-on: ubuntu-latest

        steps:
            - name: Cache Ollama installation and model files
              uses: actions/cache@v4
              with:
                # Cache Ollama binary and models directory
                path: |
                  /usr/local/bin/ollama
                  /usr/share/ollama
                  /usr/share/ollama/.ollama/models
                  ~/.ollama/models
                key: ollama-cache-${{ runner.os }}-${{ hashFiles('ollama_version.txt') }}
                restore-keys: |
                  ollama-cache-${{ runner.os }}-

            - name: Install Ollama (if not cached)
              if: steps.cache.outputs.cache-hit != 'true'
              run: |
                  curl -fsSL https://ollama.com/install.sh | sh
              
            - name: Save Ollama version
              run: |
                  ollama --version > ollama_version.txt

            - name: Run Ollama server in the background
              run: |
                  nohup ollama serve & # Start Ollama server in the background
                  sleep 10 # Give it some time to start

            - name: Run llama model
              run: |
                  ollama run llama3.1

            - name: Generate response from Ollama
              run: |
                  curl http://localhost:11434/api/generate -d '{
                    "model": "llama3.1",
                    "prompt":"Why is the sky blue? Respond with a very short answer."
                  }'
